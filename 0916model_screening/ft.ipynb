{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name=\"AXCXEPT/EZO-Common-9B-gemma-2-it\"\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
    "from unsloth import FastLanguageModel\n",
    "max_seq_length = 8192\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.9: Fast Gemma2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.15 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name, # Supports Llama, Mistral - replace this!\n",
    "    max_seq_length = max_seq_length, # Supports RoPE Scaling internally, so choose any!\n",
    "    load_in_4bit = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Get dataset\n",
    "dataset_name=\"kanhatakeyama/0717-calm3-22b-random-genre-inst-sft-tsub-part\"\n",
    "dataset = load_dataset(dataset_name, split = \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'genre': 'ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ä½œæˆ:,åºƒå‘Šã‚¹ãƒ‘ãƒ æ¤œå‡º:åºƒå‘Šç›®çš„ã®ã‚¹ãƒ‘ãƒ ã®è­˜åˆ¥,ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ã„ãŸãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹',\n",
       " 'instruction': 'å•é¡Œæ–‡ã‚’ä¸€ã¤ã®ã¿å‡ºåŠ›ã—ã¾ã™ã€‚\\n\\n---\\n\\nåºƒå‘Šã‚’ç›®çš„ã¨ã—ãŸã‚¹ãƒ‘ãƒ ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã©ã®ã‚ˆã†ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¾ã™ã‹ï¼Ÿç‰¹ã«ã€åºƒå‘ŠãŒèª¤è§£ã‚’æ‹›ãå†…å®¹ã‚’å«ã‚“ã§ã„ãªã„ã“ã¨ã‚’ä¿è¨¼ã—ã¤ã¤ã€ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã«åŸºã¥ã„ãŸå†…å®¹ã‚’ã©ã®ã‚ˆã†ã«é¸æŠã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç›Šãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æä¾›ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚',\n",
       " 'output': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ãŸã‚ã«ã¯ã€ã¾ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚„é–¢å¿ƒã‚’æ·±ãç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã“ã‚Œã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚„éå»ã®è¡Œå‹•å±¥æ­´ã€æ¤œç´¢å±¥æ­´ãªã©ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ã€ã©ã®ã‚ˆã†ãªãƒˆãƒ”ãƒƒã‚¯ã«é–¢å¿ƒãŒã‚ã‚‹ã®ã‹ã‚’æŠŠæ¡ã—ã¾ã™ã€‚æ¬¡ã«ã€ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã‚’é¸å®šã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãŒå¿…è¦ã§ã™ã€‚ã“ã‚Œã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ä¿¡é ¼æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã€å®šè©•ã®ã‚ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã‚„å°‚é–€æ©Ÿé–¢ã‹ã‚‰ã®æƒ…å ±ã‚’ä¸­å¿ƒã«åé›†ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\\n\\nå…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’è¸ã‚€ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\\n\\n1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«åŸºã¥ããƒˆãƒ”ãƒƒã‚¯ã®é¸å®š:\\n   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ã«é–²è¦§ã—ãŸè¨˜äº‹ã‚„æ¤œç´¢ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åŸºã«ã€é–¢é€£ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®šã—ã¾ã™ã€‚\\n   - ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã®ç™ºè¨€ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã‚‚å‚è€ƒã«ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé–¢å¿ƒã‚’æŒã£ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒã‚’æ·±æ˜ã‚Šã—ã¾ã™ã€‚\\n\\n2. ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã®ç‰¹å®š:\\n   - ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åé›†ã™ã‚‹éš›ã«ã¯ã€ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’è¡Œã£ã¦ã„ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã‚„ã€ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚ºãƒ ã®å€«ç†åŸºæº–ã«æº–æ‹ ã—ã¦ã„ã‚‹æ©Ÿé–¢ã‚’å„ªå…ˆã—ã¾ã™ã€‚\\n   - ä¾‹ãˆã°ã€BBCã€The New York Timesã€The Guardianãªã©ã€å›½éš›çš„ã«è©•ä¾¡ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚\\n\\n3. åºƒå‘Šã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨æ¤œè¨¼:\\n   - åºƒå‘ŠãŒèª¤è§£ã‚’æ‹›ãå†…å®¹ã‚’å«ã‚“ã§ã„ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€åºƒå‘Šä¸»ã®èƒŒæ™¯ã‚„éå»ã®å®Ÿç¸¾ã‚’èª¿æŸ»ã—ã¾ã™ã€‚\\n   - åºƒå‘Šã®å†…å®¹ãŒäº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®æƒ…å ±æºã‚’æ¯”è¼ƒæ¤œè¨ã—ã¾ã™ã€‚\\n\\n4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç›Šãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æä¾›:\\n   - é¸å®šã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ä¸­ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«æœ€ã‚‚åˆè‡´ã™ã‚‹ã‚‚ã®ã‚’é¸ã³å‡ºã—ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ¼ãƒ‰ã¨ã—ã¦æä¾›ã—ã¾ã™ã€‚\\n   - è¨˜äº‹ã®è¦ç´„ã‚„é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçŸ­æ™‚é–“ã§å†…å®¹ã‚’ç†è§£ã§ãã‚‹ã‚ˆã†ã«å·¥å¤«ã—ã¾ã™ã€‚\\n   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç¶™ç¶šçš„ã«åé›†ã—ã€æä¾›ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¿æ•´ã—ã¾ã™ã€‚\\n\\nã“ã®ã‚ˆã†ã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æä¾›ã—ã¤ã¤ã€åºƒå‘ŠãŒèª¤è§£ã‚’æ‹›ãå†…å®¹ã‚’å«ã¾ãªã„ã‚ˆã†ã«æ³¨æ„ã‚’æ‰•ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã«åŸºã¥ã„ãŸå†…å®¹ã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç›Šã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚',\n",
       " 'text': '<bos><start_of_turn>user\\nå•é¡Œæ–‡ã‚’ä¸€ã¤ã®ã¿å‡ºåŠ›ã—ã¾ã™ã€‚\\n\\n---\\n\\nåºƒå‘Šã‚’ç›®çš„ã¨ã—ãŸã‚¹ãƒ‘ãƒ ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã©ã®ã‚ˆã†ã«ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã—ã¾ã™ã‹ï¼Ÿç‰¹ã«ã€åºƒå‘ŠãŒèª¤è§£ã‚’æ‹›ãå†…å®¹ã‚’å«ã‚“ã§ã„ãªã„ã“ã¨ã‚’ä¿è¨¼ã—ã¤ã¤ã€ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã«åŸºã¥ã„ãŸå†…å®¹ã‚’ã©ã®ã‚ˆã†ã«é¸æŠã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç›Šãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æä¾›ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚<end_of_turn>\\n<start_of_turn>model\\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«åŸºã¥ã„ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ãŸã‚ã«ã¯ã€ã¾ãšãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚„é–¢å¿ƒã‚’æ·±ãç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã“ã‚Œã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æƒ…å ±ã‚„éå»ã®è¡Œå‹•å±¥æ­´ã€æ¤œç´¢å±¥æ­´ãªã©ã‚’åˆ†æã™ã‚‹ã“ã¨ã§ã€ã©ã®ã‚ˆã†ãªãƒˆãƒ”ãƒƒã‚¯ã«é–¢å¿ƒãŒã‚ã‚‹ã®ã‹ã‚’æŠŠæ¡ã—ã¾ã™ã€‚æ¬¡ã«ã€ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã‚’é¸å®šã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãŒå¿…è¦ã§ã™ã€‚ã“ã‚Œã¯ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ä¿¡é ¼æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ã€å®šè©•ã®ã‚ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã‚„å°‚é–€æ©Ÿé–¢ã‹ã‚‰ã®æƒ…å ±ã‚’ä¸­å¿ƒã«åé›†ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\\n\\nå…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’è¸ã‚€ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚\\n\\n1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«åŸºã¥ããƒˆãƒ”ãƒƒã‚¯ã®é¸å®š:\\n   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒéå»ã«é–²è¦§ã—ãŸè¨˜äº‹ã‚„æ¤œç´¢ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’åŸºã«ã€é–¢é€£ã™ã‚‹ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®šã—ã¾ã™ã€‚\\n   - ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ã§ã®ç™ºè¨€ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã‚‚å‚è€ƒã«ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé–¢å¿ƒã‚’æŒã£ã¦ã„ã‚‹ãƒ†ãƒ¼ãƒã‚’æ·±æ˜ã‚Šã—ã¾ã™ã€‚\\n\\n2. ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã®ç‰¹å®š:\\n   - ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åé›†ã™ã‚‹éš›ã«ã¯ã€ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’è¡Œã£ã¦ã„ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã‚„ã€ã‚¸ãƒ£ãƒ¼ãƒŠãƒªã‚ºãƒ ã®å€«ç†åŸºæº–ã«æº–æ‹ ã—ã¦ã„ã‚‹æ©Ÿé–¢ã‚’å„ªå…ˆã—ã¾ã™ã€‚\\n   - ä¾‹ãˆã°ã€BBCã€The New York Timesã€The Guardianãªã©ã€å›½éš›çš„ã«è©•ä¾¡ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚\\n\\n3. åºƒå‘Šã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨æ¤œè¨¼:\\n   - åºƒå‘ŠãŒèª¤è§£ã‚’æ‹›ãå†…å®¹ã‚’å«ã‚“ã§ã„ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€åºƒå‘Šä¸»ã®èƒŒæ™¯ã‚„éå»ã®å®Ÿç¸¾ã‚’èª¿æŸ»ã—ã¾ã™ã€‚\\n   - åºƒå‘Šã®å†…å®¹ãŒäº‹å®Ÿã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®æƒ…å ±æºã‚’æ¯”è¼ƒæ¤œè¨ã—ã¾ã™ã€‚\\n\\n4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç›Šãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æä¾›:\\n   - é¸å®šã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ä¸­ã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã«æœ€ã‚‚åˆè‡´ã™ã‚‹ã‚‚ã®ã‚’é¸ã³å‡ºã—ã€ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ•ã‚£ãƒ¼ãƒ‰ã¨ã—ã¦æä¾›ã—ã¾ã™ã€‚\\n   - è¨˜äº‹ã®è¦ç´„ã‚„é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’å¼·èª¿è¡¨ç¤ºã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒçŸ­æ™‚é–“ã§å†…å®¹ã‚’ç†è§£ã§ãã‚‹ã‚ˆã†ã«å·¥å¤«ã—ã¾ã™ã€‚\\n   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ç¶™ç¶šçš„ã«åé›†ã—ã€æä¾›ã™ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¿æ•´ã—ã¾ã™ã€‚\\n\\nã“ã®ã‚ˆã†ã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æä¾›ã—ã¤ã¤ã€åºƒå‘ŠãŒèª¤è§£ã‚’æ‹›ãå†…å®¹ã‚’å«ã¾ãªã„ã‚ˆã†ã«æ³¨æ„ã‚’æ‰•ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ä¿¡é ¼æ€§ã®ã‚ã‚‹æƒ…å ±æºã«åŸºã¥ã„ãŸå†…å®¹ã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦æœ‰ç›Šã§æ­£ç¢ºãªæƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚<end_of_turn>\\n<start_of_turn>model\\n<eos>'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "\n",
    "        chat = [\n",
    "            { \"role\": \"user\", \"content\": instruction },\n",
    "            { \"role\": \"assistant\", \"content\": output},\n",
    "        ]\n",
    "        text= tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)+EOS_TOKEN\n",
    "        #text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9096/9096 [00:02<00:00, 4513.81 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 9,096 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 1\n",
      "\\        /    Total batch size = 2 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      3\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m   ),\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:145\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:363\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unsloth_env/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = TrainingArguments(\n",
    "      per_device_train_batch_size = 2,\n",
    "      #gradient_accumulation_steps = 4,\n",
    "      warmup_steps = 10,\n",
    "      max_steps = 60,\n",
    "      fp16 = not torch.cuda.is_bf16_supported(),\n",
    "      bf16 = torch.cuda.is_bf16_supported(),\n",
    "      logging_steps = 1,\n",
    "      output_dir = \"outputs\",\n",
    "      optim = \"adamw_8bit\",\n",
    "      seed = 3407,\n",
    "  ),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "          (rotary_emb): GemmaFixedRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
