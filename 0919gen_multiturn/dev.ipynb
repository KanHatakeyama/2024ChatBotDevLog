{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "model_name = \"cyberagent/calm3-22b-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 16951/16951 [00:04<00:00, 3834.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds=load_dataset(\"globis-university/aozorabunko-clean\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_random_text(ds):\n",
    "    record=ds[random.randint(0,len(ds))]\n",
    "    extract_length=random.randint(100,300)\n",
    "    extract_pos=random.randint(0,len(record[\"text\"])-extract_length)\n",
    "    extracted_text=record[\"text\"][extract_pos:extract_pos+extract_length].strip()\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto reload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from vllm import LLM, SamplingParams\n",
    "pid = os.getpid()\n",
    "seed = int(pid)+int(datetime.now().timestamp())\n",
    "print(\"seed: \", seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "out_dir = \"data\"\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llm_gen(model, prompt_list,\n",
    "            temperature=0.7, top_k=50,\n",
    "            max_tokens=1024,\n",
    "            ):\n",
    "    outputs = model.generate(\n",
    "        prompt_list,\n",
    "        sampling_params=SamplingParams(\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_k=top_k,\n",
    "        )\n",
    "    )\n",
    "    return [i.outputs[0].text.strip() for i in outputs]\n",
    "\n",
    "\n",
    "tensor_parallel_size = 1\n",
    "# トークナイザーとモデルの準備\n",
    "model = LLM(\n",
    "    model=model_name,\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=2000,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "genres = [\n",
    "    \"心温まる日常会話\",\n",
    "    \"楽しい会話\",\n",
    "    \"ユーモラスな会話\",\n",
    "    \"日常会話\",\n",
    "    \"傾聴の会話\",\n",
    "    \"相手の興味をそそる会話\",\n",
    "]\n",
    "\n",
    "\n",
    "def clean_output(text):\n",
    "    if text.startswith(\"「\"):\n",
    "        text = text[1:]\n",
    "    if text.endswith(\"」\"):\n",
    "        text = text[:-1]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def ask_model(messages,model):\n",
    "    messages=copy.deepcopy(messages)\n",
    "\n",
    "    if messages[-1][\"role\"]==\"user\":\n",
    "        messages.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": \"\"})\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False,)\n",
    "    prompt=prompt.replace(\"<|im_end|>\\n\",\"\")\n",
    "    text=llm_gen(model, [prompt], temperature=0.7, top_k=50, max_tokens=1024)[0]\n",
    "    return text\n",
    "\n",
    "def gen_init_text(line, genre):\n",
    "    system_list = [\n",
    "        f\"次の文章をもとに､{genre}のはじめのセリフを出力しなさい｡対話はタメ口で行われ､敬語は使いません｡セリフのみを出力し、それ以外は何も出力しないでください｡\",\n",
    "        f\"次の文章をもとに､{genre}のはじめのセリフを出力しなさい｡対話は敬語で行われます｡セリフのみを出力し、それ以外は何も出力しないでください｡\",\n",
    "    ]\n",
    "    system_message = random.choice(system_list)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    #参考にする文章: {line}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    text=ask_model(messages,model)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text( genres, client,\n",
    "             n_replies=8):\n",
    "    genre = random.choice(genres)\n",
    "\n",
    "    seed_text=extract_random_text(ds)\n",
    "    text = gen_init_text(seed_text, genre)\n",
    "\n",
    "    system_list = [\n",
    "        f\"{genre}の返答文を生成しなさい｡セリフのみを出力し、それ以外は何も出力しないでください｡\",\n",
    "    ]\n",
    "    system_message = random.choice(system_list)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "\n",
    "    for j in range(n_replies):\n",
    "        res = ask_model(messages, client)\n",
    "        if j % 2 == 1:\n",
    "            messages.append({\"role\": \"user\", \"content\": res})\n",
    "        else:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": res})\n",
    "\n",
    "    return messages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "次の文章をもとに､心温まる日常会話のはじめのセリフを出力しなさい｡対話は敬語で行われます｡セリフのみを出力し、それ以外は何も出力しないでください｡<|im_start|>user\n",
      "\n",
      "    #参考にする文章: ○帽子は既に述べしが如く洋服の形に従つて各戴くべきものあり。背広に鳥打帽を冠るは適しからず。鳥打帽はその名の如く銃猟、旅行航海等の折にのみ用るものにて、平生都会にてこれを戴くもの巴里あたりにては職工か新聞売子なぞなるべし。欧米ともに黒の山高帽は普通一般に用ひらるるものなり。殊に米国東部の都市にては晴雨共に風甚しきが故、中折帽は吹飛ばされて不便なり。かつまた山高帽は丈夫にて雨にあたりても形崩れず、甚経済なるものなり。夏の炎天にても黒山高帽にてすこしも可笑しきことなし。中折帽は春より夏にかけて年々の流行あり。されば中折帽を冠るほどなれば洋服もこれに\n",
      "    <|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_text=extract_random_text(ds)\n",
    "prompt=gen_init_text(seed_text,random.choice(genres))\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n次の文章をもとに､心温まる日常会話のはじめのセリフを出力しなさい｡対話は敬語で行われます｡セリフのみを出力し、それ以外は何も出力しないでください｡<|im_start|>user\\n\\n    #参考にする文章: ○帽子は既に述べしが如く洋服の形に従つて各戴くべきものあり。背広に鳥打帽を冠るは適しからず。鳥打帽はその名の如く銃猟、旅行航海等の折にのみ用るものにて、平生都会にてこれを戴くもの巴里あたりにては職工か新聞売子なぞなるべし。欧米ともに黒の山高帽は普通一般に用ひらるるものなり。殊に米国東部の都市にては晴雨共に風甚しきが故、中折帽は吹飛ばされて不便なり。かつまた山高帽は丈夫にて雨にあたりても形崩れず、甚経済なるものなり。夏の炎天にても黒山高帽にてすこしも可笑しきことなし。中折帽は春より夏にかけて年々の流行あり。されば中折帽を冠るほどなれば洋服もこれに\\n    <|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "out_file_path = f\"data/gen_mult_{current_time_no_symbols}.jsonl\"\n",
    "\n",
    "with open(out_file_path, \"a\") as f:\n",
    "    for i in tqdm(range(10**4)):\n",
    "        try:\n",
    "            messages = gen_text(genres, model, n_replies=7)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        f.write(json.dumps({\"messages\": messages}, ensure_ascii=False)+\"\\n\")\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
