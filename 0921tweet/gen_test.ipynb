{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/setup/miniconda3/envs/vllm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-21 15:29:36,725\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "# %%\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import transformers\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed:  1729505828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pid = os.getpid()\n",
    "seed = int(pid)+int(datetime.now().timestamp())\n",
    "print(\"seed: \", seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "out_dir = \"data\"\n",
    "os.system(f\"mkdir -p {out_dir}\")\n",
    "\n",
    "current_time_no_symbols = datetime.now().strftime(\n",
    "    \"%Y-%m-%d %H:%M:%S\").replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000000it [00:30, 328700.89it/s]\n"
     ]
    }
   ],
   "source": [
    "target_jsonl=\"source/split_twitter_archive_cleaned_aa.jsonl\"\n",
    "tweet_list=[]\n",
    "with open(target_jsonl, \"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        tweet_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '美術の風景画出さなきゃいけないらしいのに捨てちゃった',\n",
       " 'info': 'gs://hatakeyama_data_share/ver1/eiji_takahashi/archiveteam-twitter-stream-2022-04/twitter-stream-20220405.tar',\n",
       " 'id': 1511233391974559748,\n",
       " 'in_reply_to_status_id': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 15:29:48 llm_engine.py:223] Initializing an LLM engine (v0.6.1.post2) with config: model='cyberagent/calm3-22b-chat', speculative_config=None, tokenizer='cyberagent/calm3-22b-chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=cyberagent/calm3-22b-chat, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=True)\n",
      "INFO 09-21 15:29:51 model_runner.py:997] Starting to load model cyberagent/calm3-22b-chat...\n",
      "INFO 09-21 15:29:52 weight_utils.py:242] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/10 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 1/10 [00:00<00:01,  6.33it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 2/10 [00:00<00:03,  2.16it/s]\n",
      "Loading safetensors checkpoint shards:  30% Completed | 3/10 [00:01<00:04,  1.68it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 4/10 [00:02<00:03,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 5/10 [00:02<00:03,  1.52it/s]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 6/10 [00:03<00:02,  1.48it/s]\n",
      "Loading safetensors checkpoint shards:  70% Completed | 7/10 [00:04<00:02,  1.45it/s]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 8/10 [00:05<00:01,  1.45it/s]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 9/10 [00:05<00:00,  1.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:06<00:00,  1.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 10/10 [00:06<00:00,  1.55it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-21 15:29:59 model_runner.py:1008] Loading model weights took 41.9933 GB\n",
      "INFO 09-21 15:30:00 gpu_executor.py:122] # GPU blocks: 1616, # CPU blocks: 227\n",
      "INFO 09-21 15:30:02 model_runner.py:1311] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 09-21 15:30:02 model_runner.py:1315] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-21 15:30:34 model_runner.py:1430] Graph capturing finished in 32 secs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"cyberagent/calm3-22b-chat\"\n",
    "tensor_parallel_size = 1\n",
    "# トークナイザーとモデルの準備\n",
    "model = LLM(\n",
    "    model=model_name,\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=2000,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "export CUDA_VISIBLE_DEVICES=0\n",
    "export CUDA_VISIBLE_DEVICES=1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_random_text(ds):\n",
    "    seed = int(pid)+int(datetime.now().timestamp())\n",
    "    random.seed(seed)\n",
    "    record = ds[random.randint(0, len(ds))]\n",
    "    extract_length = random.randint(100, 300)\n",
    "    try:\n",
    "        extract_pos = random.randint(0, len(record[\"text\"])-extract_length)\n",
    "        extracted_text = record[\"text\"][extract_pos:extract_pos +\n",
    "                                        extract_length].strip()\n",
    "    except:\n",
    "        extracted_text = record[\"text\"][0:100].strip()\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "\n",
    "# %%\n",
    "# auto reload modules\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "max_tokens = 1024\n",
    "\n",
    "\n",
    "def llm_gen(model, prompt_list,\n",
    "            temperature=0.7, top_k=50,\n",
    "            max_tokens=max_tokens,\n",
    "            ):\n",
    "    outputs = model.generate(\n",
    "        prompt_list,\n",
    "        sampling_params=SamplingParams(\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_k=top_k,\n",
    "            stop=\"\\n\\n\", # 2回改行が出たら終了｡会話の二周目以降なので｡\n",
    "        )\n",
    "    )\n",
    "    return [i.outputs[0].text.strip() for i in outputs]\n",
    "\n",
    "def clean_output(text):\n",
    "    if text.startswith(\"「\") and text.endswith(\"」\"):\n",
    "        text = text[1:-1]\n",
    "    return text\n",
    "\n",
    "\n",
    "# %%\n",
    "def ask_model(messages, model):\n",
    "    messages = copy.deepcopy(messages)\n",
    "\n",
    "    if messages[-1][\"role\"] == \"user\":\n",
    "        messages.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": \"\"})\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False,)\n",
    "    prompt = prompt.replace(\"<|im_end|>\\n\", \"\")\n",
    "    text = llm_gen(model, [prompt], temperature=0.7,\n",
    "                   top_k=50, max_tokens=1024)[0]\n",
    "    return text\n",
    "\n",
    "genres = [\n",
    "    \"心温まる日常会話\",\n",
    "    \"楽しい会話\",\n",
    "    \"ユーモラスな会話\",\n",
    "    \"日常会話\",\n",
    "    \"傾聴の会話\",\n",
    "    \"相手の興味をそそる会話\",\n",
    "    \"長めの会話\",\n",
    "    # \"指示を聞いて答えるタイプの会話\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gen_reply(tweet, genre):\n",
    "    system_list = [\n",
    "        f\"あなたはSNSで{genre}をしています｡口調やノリを相手に合わせながら､返事をしてください｡返答のみを出力し、それ以外は何も出力しないでください｡\",\n",
    "    ]\n",
    "    system_message = random.choice(system_list)\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": tweet},\n",
    "    ]\n",
    "    text = ask_model(messages, model)\n",
    "    text = clean_output(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_prompt(tweet_list, genres):\n",
    "    genre = random.choice(genres)\n",
    "    tweet= random.choice(tweet_list)[\"text\"]\n",
    "    system_list = [\n",
    "        f\"あなたはSNSで{genre}をしています｡口調やノリを相手に合わせながら､返事をしてください｡返答のみを出力し、それ以外は何も出力しないでください｡\",\n",
    "    ]\n",
    "    system_message = random.choice(system_list)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": tweet},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False,)\n",
    "    prompt = prompt.replace(\"<|im_end|>\\n\", \"\")\n",
    "    return prompt,tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 100/100 [00:03<00:00, 29.56it/s, est. speed input: 2032.94 toks/s, output: 554.28 toks/s]\n",
      "  0%|          | 0/50000 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size=300\n",
    "\n",
    "\n",
    "out_file_path = f\"data/reply_{current_time_no_symbols}.jsonl\"\n",
    "\n",
    "with open(out_file_path, \"a\") as f:\n",
    "    for i in tqdm(range(5*10**4)):\n",
    "        try:\n",
    "            prompts,tweets=zip(*[gen_prompt(tweet_list, genres) for i in range(batch_size)])\n",
    "            replies= llm_gen(model, prompts, temperature=0.7,\n",
    "                            top_k=50, max_tokens=256)\n",
    "            replies=[clean_output(i) for i in replies]\n",
    "            for prompt,tweet,reply in zip(prompts,tweets,replies):\n",
    "                messages = [\n",
    "                    #{\"role\": \"system\", \"content\": prompt},\n",
    "                    {\"role\": \"user\", \"content\": tweet},\n",
    "                    {\"role\": \"assistant\", \"content\": reply},\n",
    "                ]\n",
    "                f.write(json.dumps({\"messages\": messages}, ensure_ascii=False)+\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
